{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### K-nearest neighbours \n",
    "\n",
    "##### Application:\n",
    "\n",
    "Is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure. KNN is mostly useful in situations where the solution is centered around identifying similarities between data points to associate them in the same class.\n",
    "\n",
    "##### Strenghts:\n",
    "\n",
    "+KNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset. This will be very helpful in practice where most of the real world datasets do not follow mathematical theoretical assumptions.\n",
    "+The algorithm is simple and easy to implement.\n",
    "+There’s no need to build a model, tune several parameters, or make additional assumptions.\n",
    "+The algorithm is versatile. It can be used for classification, regression, and search (as we will see in the next section)\n",
    "\n",
    "##### Weaknesses:\n",
    "\n",
    "-KNN performs better with a lower number of features than a large number of features. You can say that when the number of features increases than it requires more data. Increase in dimension also leads to the problem of overfitting. To avoid overfitting, the needed data will need to grow exponentially as you increase the number of dimensions. This problem of higher dimension is known as the Curse of Dimensionality.\n",
    "\n",
    "-The algorithm gets significantly slower as the number of examples and/or predictors/independent variables increase.\n",
    "\n",
    "##### Why apply it to this probblem:\n",
    "\n",
    "Overall although this algorithm would be suitable from the perspective of non-parametric assumptions about the data, I will not employ it due to the large number of predictor features and the fact that in order to arrive at reasonable predictions with this amount of features the algorithm would require a much higher number of records. The current number of records in the train set is 36177 and the number of features is 103\n",
    "\n",
    "+ source: https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn\n",
    "+ source: https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761#:~:text=Summary,that%20data%20in%20use%20grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent Classifier\n",
    "The disadvantage of the Stochastic Gradient Descent Classifier is that it is sensitive to feauture scaling. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes (GaussianNB)\n",
    " \n",
    "##### Application:\n",
    "\n",
    "##### Strenghts:\n",
    " \n",
    "- Naive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods.\n",
    "- Naive Bayes algorithms are quite intuitive and therefore easy to explain in Layman's terms\n",
    " \n",
    "##### Weaknesses:\n",
    "\n",
    "- The likelihood of the features is assumed to be Gaussian.\n",
    "- When dealing with continuous data, a typical assumption is that the continuous values associated with each class are distributed according to a normal (or Gaussian) distribution. Since this is not the case for our features as we have seen in the example above with capital gain and capital loss, I do not consider Gaussian NB to be the most optimal algorithm to try out here.\n",
    "\n",
    "\n",
    "##### Why apply it to this probblem:\n",
    " \n",
    "\n",
    "+ source: https://en.wikipedia.org/wiki/Naive_Bayes_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier:¶\n",
    "\n",
    "##### Application:\n",
    "\n",
    "- Xbox Kinnect, where Random Forest is used for Real time Human Pose Recognition.\n",
    "\n",
    "##### Strenghts:\n",
    "\n",
    "- Scale quickly, have ability to deal with unbalanced and missing data\n",
    "- Generates an internal unbiased estimate of generalization error as forest building progresses.\n",
    "- Provides an expeimental way to detect variable interactions.\n",
    "\n",
    "##### Weaknesses:\n",
    "\n",
    "- Less effective on noisier-larger datasets with overlapping classes.\n",
    "- large number of trees may lead to slow real-time prediction in some cases.\n",
    "\n",
    "##### Why apply it to this probblem:\n",
    "\n",
    "- RandomForest are always a safe bet as they generally have high average accuracy rate for most cases and work well for complex classification tasks as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier:¶\n",
    "\n",
    "##### Application:\n",
    "\n",
    "- Higgs Boson Discovery from the Large Hadron Collider dataset. Physicits can extract signal of Higgs Boson particle from background noises which potentially leading to major breakthroghs in modern physics.\n",
    "\n",
    "##### Strenghts:\n",
    "\n",
    "- It builds new trees which complement the already built trees. The new trees which will be built will help to correct errors in the previously built trees. This can produce highly accurate results with less trees.\n",
    "- Can handle different types of predictor variables and accomodate missing data.\n",
    "\n",
    "##### Weaknesses:\n",
    "\n",
    "- Unable to compute conditional class probabilites\n",
    "- Suffers from long sequential computation times.\n",
    "- More parameters to tune.\n",
    "\n",
    "##### Why apply it to this probblem:\n",
    "\n",
    "- Since, this model produces good accurate results, this makes it a very good candidate for the problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "(https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/)\n",
    "\n",
    "##### Application:\n",
    "- Calculate the probability that someone will vote Democrat or Republican\n",
    "\n",
    "##### Strengths: \n",
    "- Offers probabilities of classification as opposed to discrete classifications which can be useful\n",
    "- Generalizes well\n",
    "\n",
    "##### Weaknesses: \n",
    "- Doesn't handle large number of features well\n",
    "- Tends to underfit the model to the data\n",
    "\n",
    "##### Why apply it to this probblem:\n",
    "- This is a classification algorithm, given we are trying to determine income above or below 50k it is suitable. Upon review I now think this may not have been an optimal choice for this dataset as the number of features could be too large for this model. However as it was my initial choice I have kept it the same and learned from this. Also from interest I tested different models in the initial model evaluation and saw that Logistic Regression still performed well relative to others, therefore in practice it could be an okay choice but in theory maybe not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric to base decision on should be Precision over Recall due to the fact that FP cases will cost money to approach the people. Recall is less important but should also be taken into account because False Negatives lead to lost opportunities. \n",
    "\n",
    "Accuracy is not the best metric since the predicted value labels are not well balanced and the people with >50k income comprise just 24 % of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity3.8",
   "language": "python",
   "name": "udacity3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
